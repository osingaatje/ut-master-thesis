@InProceedings{Ahmed2024,
  author     = {Faizan Ahmed and Nacir Bouali and Marcus Gerhold},
  title      = {Teaching Assistants as Assessors: An Experience Based Narrative},
  year       = {2024},
  abstract   = {This study explores the role of teaching assistants (TAs) as assessors in a university’s computer science pro-
gram. It examines the challenges and implications of TAs in grading, with a focus on their expertise and
grading consistency. The paper analyzes grading experiences in various exam settings and investigates the
impact on assessment quality. We adopt an empirical methodology and answer the research question by an-
alyzing the data from two exams. The chosen exams have similar learning objectives but they differ in how
TAs graded them, thus providing an opportunity to reflect on different grading styles. It concludes with
recommendations for enhancing TA grading effectiveness, emphasizing the need for detailed rubrics, training,
and monitoring to ensure fair and reliable assessment in higher education.},
  comment    = {Groups of TAs (UTA, GTA, Ph.D). Helpful in scaling program size, but must be carefully evaluated and intervened by providing necessary training.
"TA evaluation should be standardized with (...) scrutiny to prevent outliers"

p.122: Best arrangement of TA grading: one question per TA, all TAs in one room to discuss, teachers must monitor closely, mix of experienced and junior TAs. Students should be present in review to flag wrong grading etc.

Also: p.122 "If mistakes are spotted during [a] review, audit all exams graded by the TA for whom a mistake was spotted"

Grading rubric in Figure 3 seems very appropriate for autograding!},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/ahmed2024_TAs-experience-based-narrative.pdf:PDF},
  groups     = {NOT autograding},
  keywords   = {Teaching Assistants, Grading Consistency, Grading Variation, Reliability},
  readstatus = {read},
  url        = {https://research.utwente.nl/files/457355611/126242.pdf},
}

@InProceedings{Hosseinibaghdadabadi2023,
  author     = {Mohsen Hosseinibaghdadabadi and Omar Alam Nicolas Almerge and Jörg Kienzle},
  booktitle  = {2023 ACM/IEEE 26th International Conference on Model Driven Engineering Languages and Systems (MODELS)},
  title      = {Automated Grading of Use Cases},
  year       = {2023},
  publisher  = {IEEE},
  abstract   = {2023 ACM/IEEE 26th International Conference on Model Driven Engineering Languages and Systems (MODELS);2023; ; ;10.1109/MODELS58315.2023.00029},
  comment    = {Method: comparing to teacher's solution, using structural matching, syntactic/semantic word matching, NLP for sentence matching, flattening of use case hierarchies.

Levels of use cases: 'summary-level', 'user-goal level', 'subfunction-level'.

Word matching: Levenshtein distance (syntactic) + WordNet Similarity (using HSO, WUP, LIN metrics for semantic)

Sentence matching: Part-Of-Speech-analysis (tokenising, throwing away some prepositions and NER words, and then using lemmatisation to convert different version of words into one (p.109)). Interesting matching algo in Alg.3 (p.109).

Not very good research questions. Sad.

I also feel that this focuses a bit more on statistics than individual cases. Checking features and comparing flaws in grading and refining the grading rubric and tool should probably be done.

Also highlights edge cases where use case is split into two, or when two use cases are handed in by a student as one. Interesting problem to potentially solve.

Has as source Bian et al. 2019.},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/hosseinibaghdadabadi2023_AutomatedGradingOfUseCases.pdf:PDF},
  groups     = {Algorithmic},
  readstatus = {read},
  url        = {https://ieeexplore.ieee.org/iel7/10343461/10343549/10343598.pdf},
}

@InProceedings{anas2021,
  author     = {Outair Anas and Tanana Mariam and Lyhyaoui Abdelouahid},
  title      = {New method for summative evaluation of UML class diagrams based on graph similarities},
  year       = {2021},
  comment    = {Convert UML Class diagrams into graphs, use graph similarity algorithms.

Very mathematically intense paper, not nice to read.

However, the matching seems quite similar. Similarity scores of 83%, 98%, 96%. However, this is still based on a teacher example, and manual review is still required (although it is sped up because the tool automatically shows discrepancies in graphing).},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/anas2021_EvaluationUMLUsingGraphSimilarities.pdf:PDF},
  groups     = {Semi-automatic},
  keywords   = {teachers. To achieve this objective, we must analyze these productions and study the transformation, matching, similarity measurement and comparison},
  readstatus = {read},
  url        = {https://www.academia.edu/download/66135135/70_22270_EM_26aug_20feb_L.pdf},
}

@InProceedings{batmaz2010,
  author     = {Firat Batmaz},
  title      = {Semi-Automatic Assessment ofStudents’ Graph-Based Diagrams},
  year       = {2010},
  comment    = {Very in-depth methods of manual, semi-automatic grading. 
COOL: relation between explanation text and graphs/models.

Big document (236 pages)

Takes a broader look at the process of grading, identifying and developing techniques to reduce repetitive actions.},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/Batmaz2001_Semi-AutomaticAssessmentGraph-BasedDiagrams.pdf:PDF},
  groups     = {Semi-automatic},
  readstatus = {skimmed},
  url        = {https://www.academia.edu/download/66135135/70_22270_EM_26aug_20feb_L.pdf},
}

@InProceedings{Bian2019,
  author     = {Weiyi Bian and Omar Alam and Jörg Kienzle},
  booktitle  = {2019 ACM/IEEE 22nd International Conference on Model Driven Engineering Languages and Systems Companion (MODELS-C)},
  title      = {Automated Grading of Class Diagrams},
  year       = {2019},
  month      = sep,
  pages      = {700--709},
  publisher  = {IEEE},
  abstract   = {Drawing UML diagrams, such as class diagrams, is incorrect, a diagram design problem involving class diagrams
an essential learning task in many software engineering courses. can have a large solution space. For example, solutions can
In course assignments, students are tasked to draw models vary based on the class names, i.e., a student’s solution can
that describe scenarios, model requirements, or system designs.
The course instructor usually grades the diagrams manually by use a synonym for a class name instead of the exact name used},
  comment    = {Propose metamodel that maps a teacher solution with syntactic, semantic, structural matching.

Basis of the methodology to Hosseinibaghadadadi et al., 2023

TWO MAIN CONTRIBUTIONS:
1. Metamodel to establish mappings between instructor solution and student solution.
2. Metamodel to grade model elements.

- syntactic matching (eliminating spelling
mistakes), 
- semantic matching (considering synonyms and
words with related meaning) , 
- structural matching (matching by comparing the contents of a class, similarity based on the associations with other classes, and considering classes that are split or merged), 
- handle class inheritance (handle the class elements that are misplaced within the inheritance hierarchy), 
- match associations (including finding potential derivative associations)},
  doi        = {10.1109/models-c.2019.00106},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/bian2019-AutomatedGradingOfClassDiagrams.pdf:PDF},
  groups     = {Algorithmic, incredible!},
  keywords   = {automated grading, class diagrams, model comparison},
  priority   = {prio1},
  readstatus = {read},
  url        = {https://research.utwente.nl/files/457355611/126242.pdf},
}

@InProceedings{Bian2020,
  author     = {Bian, Weiyi and Alam, Omar and Kienzle, Jörg},
  booktitle  = {Proceedings of the 23rd ACM/IEEE International Conference on Model Driven Engineering Languages and Systems},
  title      = {Is automated grading of models effective?: assessing automated grading of class diagrams},
  year       = {2020},
  month      = oct,
  pages      = {365--376},
  publisher  = {ACM},
  series     = {MODELS ’20},
  abstract   = {science programs in record numbers [59]. As a result, automated
Learning how to model the structural properties of a problem do- grading is increasingly popular in Computer Science. Many ap-
main or an object-oriented design in the form of a class diagram is proaches to automatically assess programming assignments have
an essential learning task in many software engineering courses. been proposed and are currently being used in the classroom [16,
Since the grading of models is a time-consuming activity, automated 37]. As the class sizes also increase in advanced undergraduate
grading approaches have been developed to assist the instructor by software engineering classes, we now face the need to determine
speeding up the grading process, as well as ensuring consistency effective algorithms for automating the grading of models.
and fairness for large classrooms. This paper empirically evaluates In addition, automated grading is crucial for e-learning and
the efficacy of one such automated grading approach when applied Massive Online Open Courses (MOOCs) [20, 21]. Large numbers
in two real world settings: a beginner undergraduate class of 103 of students, sometimes thousands, enroll in these online courses.
students required to create an object-oriented design model, and an Therefore, it becomes difficult to manually grade their assignments
advanced undergraduate class of 89 students elaborating a domain and exams. Furthermore, in MOOCs, students need tools to self-
model. The results of the experiment highlight a) the need to adapt assess their knowledge to decide whether they want to move to
the grading strategy and strictness to the level of the students and the next topic in the course. Such self-assessment methods have
the grading style of the instructor, and b) the importance of con- been implemented in popular online learning platforms, e.g. Khan
sidering multiple solution variants when grading. Modifications to Academy [5], Udemy [7], and Coursera [2]. Automated assessment
the grading algorithm are proposed and validated experimentally. can also be used to calibrate a learner’s prior knowledge [42], i.e.,},
  collection = {MODELS ’20},
  comment    = {Study of grading tool on UML diagrams. Main findings:
- multiple teacher solutions are effective if you use a tool that compares student solutions to teacher solutions ("40/89 students used an alternative correct solution in their assignment", p.12)

- grading configurations (deductions of certain points) change per exam if you want similar grades to a teacher's solution (the focus is likely on different parts of UML per exam)

- autograding helps ensure fairness and trasparency (within 9% of manual grading with algorithmic, deterministic evaluation, "more consistent and able to ensure fairness in the grading process").

- "incremental discovery of alternative solutions" would be nice to have, since a student might give a solution that the teacher thinks is cool, but the autograder does not take this into account (unless program can regrade using the (adjusted) student solution)},
  doi        = {10.1145/3365438.3410944},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/bian2020-automated-class-diagram-grading.pdf:PDF},
  groups     = {Algorithmic, incredible!},
  priority   = {prio1},
  readstatus = {read},
  url        = {https://dl.acm.org/doi/pdf/10.1145/3365438.3410944},
}

@InProceedings{Bouali2025,
  author     = {Nacir Bouali and Marcus Gerhold and Tosif Ul Rehman and Faizan Ahmed},
  title      = {Toward Automated UML Diagram Assessment: Comparing LLM-Generated Scores with Teaching Assistants},
  year       = {2025},
  abstract   = {This paper investigates the feasibility of using Large Language Models (LLMs) to automate the grading of
Unified Modeling Language (UML) class diagrams in a software design course. Our method involves care-
fully designing case studies with constraints that guide students’ design choices, converting visual diagrams
to textual descriptions, and leveraging LLMs’ natural language processing capabilities to evaluate submis-
sions. We evaluated our approach using 92 student submissions, comparing grades assigned by three teaching
assistants with those generated by three LLMs (Llama, GPT o1-mini, and Claude). Our results show that
GPT o1-mini and Claude Sonnet achieved strong alignment with human graders, reaching correlation coef-
ficients above 0.76 and Mean Absolute Errors below 4 points on a 40-point scale. The findings suggest that
LLM-based grading can provide consistent, scalable assessment of UML diagrams while matching the grading
quality of human assessors. This approach offers a promising solution for managing growing student numbers
while ensuring fair and timely feedback.},
  comment    = {Grading UML class diagrams with LLMs + sentence transformers and structured grading rubric.

Challenges in automated grading: 
- semantic equivalence (p.159), 
- different student interpretations - partially solved by tools such as TouchCORE, but "require significant instructor effort to organize acceptable variations."
- balancing accuracy with model completeness - systems "often over-penalize missing elements or fail to (...) assign partial grades"
- "LLMs could be valuable tool in automating the grading" p.158
- "current systems often lack the granularity (...) [to] assess incomplete submissions"
- Rule-based grading is fast but are "not very efficient due to rigid rules, (...) considerable configuration effort, and the struggle to handle diverse or creative solutions" - ML predicts scores from trained datasets, but challenges: data dependency, difficulty in interpretation, high computational cost
	- LLMs handle creative solutions but "face challenges domain-specific fine-tuning and ethical concerns" (bias/transparency)
- combining methods (Boubekeur et al., 2020) could balance flexibility and performance

Methodology:
- LLMs with 0 temperature (deterministic)

Grading rubric is provided which looks a whole lot like a ruleset for an autograder! I wonder if we can combine the ruleset with a teacher submission / derive rulesets from a teacher's submission and grade that way... (although the pure class-similarity approach of Bian 2019 looks more flexible at face value...)

"All three LLMs (...) ability to process and *understand*" big red flag here, ascribing human traits to machines.
"While the models would provide a final score as requested in the prompt’s response format, this score often did not match the actual sum of points awarded in their criterion-by-criterion assessment"

"The near parity between human graders and LLMs highlights their potential to significantly reduce grading workloads" - " LLMs can effectively grade these subjective tasks" <-- they are not fully objective, but far from completely subjective. There is no proof in this paper given about the transparency and clarity of grading.


"While the models would provide a final score as requested in the prompt’s response format, this  core often did not match the actual sum of points awarded in their criterion-by-criterion assessment. This discrepancy can be attributed to the autoregressive nature of LLMs, where they generate responses token by token without maintaining perfect consistency across long outputs" (p.164)},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/bouali2025_LLM-Grading-UML-compared-to-TAs.pdf:PDF},
  groups     = {Review, LLM},
  keywords   = {AI-Assisted Grading, Autograding, Large Language Models, GPT, Llama, Claude, UML},
  readstatus = {read},
  url        = {https://research.utwente.nl/files/496461589/134819.pdf},
}

@InProceedings{Stikkolorum2019,
  author     = {Dave R. Stikkolorum and Peter van der Putten and Caroline Sperandio and Michel R.V. Chaudron},
  title      = {Towards Automated Grading of UML Class Diagrams with Machine Learning},
  year       = {2019},
  abstract   = {This paper describes an exploratory study on the application
of machine learning for the grading of UML class diagrams. Bachelor stu-
dent pairs performed a software design task for learning software design
with the use of UML class diagrams. After experts had manually graded
the diagrams, we trained a regression model and multiple classification
models. Based on the results we conclude that prediction of a 10 point
grading scale can’t be done reliably. Classifying with trained data using
expert consensus and a rubric comes closer to accuracy, but is still not
good enough (a precision of 69%). Future work should include larger
training sets and an exploration for other features.},
  comment    = {expert grading vs. ML-based grading. Converting files to XMI (XML-based).

don't really say anything other than spelling mistakes and '42.76%' for the actual grading, and if you quantise the grading, then it correlates more (yeah of course, you group small errors into one group).},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/chaudron2019_TowardsAutomatedGradingWithMachineLearning.pdf:PDF},
  groups     = {LLM, bad/not worth reading},
  keywords   = {automated grading · software design · class diagrams · UML},
  readstatus = {skimmed},
  url        = {https://ceur-ws.org/Vol-2491/paper80.pdf},
}

@InProceedings{Foss2022,
  author     = {Foss, Sarah and Urazova, Tatiana and Lawrence, Ramon},
  booktitle  = {Proceedings of the 25th International Conference on Model Driven Engineering Languages and Systems: Companion Proceedings},
  title      = {Learning UML database design and modeling with AutoER},
  year       = {2022},
  month      = oct,
  pages      = {42--45},
  publisher  = {ACM},
  series     = {MODELS ’22},
  abstract   = {use modeling constructs properly. Instructors are limited on the
Designing models for software systems is a complex activity that time that they can provide for feedback to help students learn.
takes time and practice to master. Supporting students learning de- This work demonstrates a system for teaching database design
sign is time-consuming, and instructors are limited on the amount by automatically generating and marking questions. The database
of feedback they can provide. This work demonstrates a learning design problem involves translating a written text describing a do-
system that helps students learn UMLmodeling to produce database main into a database model in either ER or UML notation. Students
designs. Important features of the system are its ability to auto- struggle with the key tasks of identifying important concepts in the
evaluate design diagrams based on instructor criteria, and automat- domain description and mapping them to the appropriate model
ically generate question variants allowing students to practice on constructs.
their own. The system is integrated into the PrairieLearn learning There have been prior publications on developing modeling tools
management system allowing for its deployment at other institu- that support auto-grading [4, 5, 14]. The system demonstrated,
tions. The demonstration shows the random question generation, AutoER [11], has the following unique contributions:
how students construct database designs, and the auto-marking ca-
pabilities. The system and practice questions are publicly available • Supports students building a design model by directly inter-},
  collection = {MODELS ’22},
  comment    = {good related works (UML tooling etc.

Extension to PrairieLearn (bad UX but cool idea, making DB diagrams and grading them)

not great research. doesn't provide source code as well even though it says there is some :(},
  doi        = {10.1145/3550356.3559091},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/foss2022_AutoER_automated_database_grading.pdf:PDF},
  groups     = {Algorithmic},
  keywords   = {database design,models,education,automatic marking},
  priority   = {prio3},
  readstatus = {read},
  url        = {https://dl.acm.org/doi/pdf/10.1145/3550356.3559091},
}

@InProceedings{Foss2022b,
  author     = {Sarah Foss},
  title      = {AutoER: A System for the Automatic Generation and Evaluation of UML Database Design Diagrams},
  year       = {2022},
  comment    = {in-depth explanation of Foss2022(a)

is meant for interactive diagram making based on text.},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/foss-sarah-2020_automated-generation-evaluation-UML-database-diag.pdf:PDF},
  groups     = {Algorithmic},
  priority   = {prio3},
  readstatus = {skimmed},
  url        = {https://open.library.ubc.ca/media/download/pdf/24/1.0421624/4},
}

@InProceedings{Herout2016,
  author     = {Pavel Herout and Premek Brada},
  booktitle  = {2016 IEEE 29th International Conference on Software Engineering Education and Training (CSEET)},
  title      = {UML-Test Application for Automated Validation of Students’ UML Class Diagram},
  year       = {2016},
  month      = apr,
  pages      = {222--226},
  publisher  = {IEEE},
  abstract   = {2016 IEEE 29th International Conference on Software Engineering Education and Training (CSEET);2016; ; ;10.1109/CSEET.2016.33},
  doi        = {10.1109/cseet.2016.33},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/hernout2007_UML-Test_Application_for_Automated_Validation_of_Students_UML_Class_Diagram.pdf:PDF},
  groups     = {Slightly relevant},
  keywords   = {OOP, UML, Class diagram, automatic validation},
  readstatus = {read},
  url        = {https://ieeexplore.ieee.org/iel7/7474421/7474423/07474487.pdf},
}

@InProceedings{Jebli2023,
  author     = {Rhaydae Jebli and Jaber El Bouhdidi and Mohamed Yassin Chkouri},
  booktitle  = {2023 7th IEEE Congress on Information Science and Technology (CiSt) |},
  title      = {Assessing Students' UML Class Diagrams: a NewAutomated Solution},
  year       = {2023},
  publisher  = {IEEE},
  abstract   = {2023 7th IEEE Congress on Information Science and Technology (CiSt);2023; ; ;10.1109/CiSt56084.2023.10409936},
  comment    = {view of some tool with some (useful) heuristics. No mass verification or whatever, so meh.},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/jebli2023_AssessingStudentsUMLClassDiagrams.pdf:PDF},
  groups     = {Algorithmic},
  keywords   = {Automatic Assessment, UML diagrams, Distance assessment of students’ productions. Section III describes our},
  priority   = {prio3},
  readstatus = {read},
  url        = {https://ieeexplore.ieee.org/iel7/10409867/10409868/10409936.pdf},
}

@InProceedings{BilalKarasneh,
  author     = {Bilal Karasneh, Dave Stikkolorum, Enrique Larios, and Michel R.V. Chaudron},
  title      = {Quality Assessment of UML Class Diagrams},
  abstract   = {In this paper, we present an experiment conducted for comparing how},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/BilalKarasneh - Quality Assessment of UML Class Diagrams.pdf:PDF},
  groups     = {NOT autograding, Slightly relevant},
  keywords   = {Empirical Studies; Software Engineering Education; Software De},
  readstatus = {read},
  url        = {https://ceur-ws.org/Vol-1555/6.pdf},
}

@InProceedings{Modi2021,
  author     = {Salisu Modi and Hanan A. Taher and Hoger Mahmud},
  title      = {A Tool to Automate Student UML diagram Evaluation},
  year       = {2021},
  abstract   = {Unified modelling language (UML) is the accepted standard and modelling language for modeling in software development},
  comment    = {Java-based class diagram checker tool. Focuses more on UI and feedback. Focuses input on two ZIP files containing XMI Modeli class diagrams. Not really useful for me.},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/Modi-et-al-AToolToAutomateStudentUMLdiagramEvaluation.pdf:PDF},
  groups     = {Algorithmic},
  keywords   = {Unified Modeling Language (UML), Use case Diagram, Extensible Markup Language (XML), UML Auto},
  priority   = {prio3},
  readstatus = {read},
  url        = {https://www.academia.edu/download/72488756/575.pdf},
}

@InProceedings{Intelligence1479,
  author     = {Irina-Gabriela Nedelcu and Veronica Opranescu and Beatrice-Nicoleta Chiriac and Anca Daniela Ionita},
  title      = {Educational Support for AutomatedClassification of UML Diagrams Using MachineLearning},
  year       = {1479},
  editor     = {Angelo Sifaleras and Fuhua Lin},
  pages      = {185-192},
  publisher  = {Springer},
  comment    = {use machine learning, with their own feature extraction. Only for detection. Not useful for me.},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/nedelcu2024_ClassificationOfUMLDiagramsUsingMachineLearning.pdf:PDF},
  groups     = {NOT autograding},
  readstatus = {read},
  url        = {https://link.springer.com/chapter/10.1007/978-3-031-63031-6_16},
}

@InProceedings{Ali2007,
  author     = {Noraida Haji Ali and Zarina Shukur and Sufian Idris},
  title      = {Assessment System For UML Class DiagramUsing Notations Extraction},
  year       = {2007},
  comment    = {don't say anythign about the actual implementation and techniques.},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/noraida2007_AssessmentSystemForUMLClassDiagram.pdf:PDF},
  groups     = {Bad-ish},
  keywords   = {: class diagram. The class diagram includes all information Object-oriented Modeling, UML Class Diagram, Rational Rose about the system structure such as class object and relations and Petal File. between those classes. This paper discusses the extraction},
  readstatus = {read},
  url        = {https://www.researchgate.net/profile/Zarina-Shukur/publication/253243639_Assessment_System_For_UML_Class_Diagram_Using_Notations_Extraction/links/55487af30cf2b0cf7acec2e4/Assessment-System-For-UML-Class-Diagram-Using-Notations-Extraction.pdf},
}

@InProceedings{Ali2007b,
  author     = {Noraida Haji Ali and Zarina Shukur and Sufian Idris},
  booktitle  = {2007 International Conference on Computational Science and its Applications (ICCSA 2007)},
  title      = {A Design of an Assessment System for UML Class Diagram},
  year       = {2007},
  month      = aug,
  pages      = {539--546},
  publisher  = {IEEE},
  abstract   = {1.   Introduction
  
 Subjects such as software engineering or systems 
The Unified Modeling Language (UML) is probably analysis and design, use visual tools to describe 
the most widely known and used notation for object- software models. Normally, model will be described 
oriented analysis and design. UML consists of various through shapes, graphic symbol, relation and 
graphical notations, which capture the static system explanation in texts. This model is used to get the 
structures (class diagrams), system component prediction of cost or budget and time requirement to 
behaviors (state transition diagrams) and system finish a system. The main objective in software 
component interactions (collaboration and sequence engineering discipline is to determine a method that 
diagrams). UML notations can be produced with the can support system complexity and reduce the error in 
help of CASE (Computer-aided software engineering) software development processes. Thus, studies on 
tools such as Rational Rose. Basically, we proposed complex concepts, languages, techniques or tools to 
the development of an Assessment system for UML fulfill this requirement have started long ago. Until 
class diagram, the UML Class Diagram Assessor now, the attention has been given to the acceptance of 
(UCDA). This tool will  receive a students UML class this approach in software engineering that can be used 
diagram in the form of Rational Rose petal files. In this and applied in model process for system development.  
paper we present a design of UML Class Diagram  
Assessor (UCDA) that evaluates UML class diagrams Over the years, innumerable analysis and design 
automatically. UCDA evaluates the diagram based on methods and notations have been proposed and 
three aspects: its structure; its correctness and developed. In late 1997, most of these methods have 
language used. The output of UCDA is a list of converged to a standard notation, the Unified Modeling 
comments on a diagram that is hoped to guide students Language (UML), which has been accepted as an 
in understanding on how to represent the system industry standard by the OMG (Object Management 
requirement in UML model correctly.  Group, 1999) [1]. CASE tools, which provide support 
 for UML diagramming (e.g. Rational Rose, Microsoft 
 Visio and Enterprise Architect) can benefit from the 
 use of an automatic layout tool. The first attempts at 
 using computers to automate the process of assessing 
 student work were reported in the early 1960’s [2].},
  comment    = {same as previous Ali et al. (2007) work.},
  doi        = {10.1109/iccsa.2007.2},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/noraida2007_Design_of_Assessment_System_for_UML_Class_Diagram.pdf:PDF},
  groups     = {Bad-ish},
  readstatus = {read},
}

@TechReport{thomas2006,
  author     = {Pete Thomas and Neil Smith and Kevin Waugh},
  title      = {An approach to the automatic grading of imprecise diagrams},
  year       = {2006-12-01},
  comment    = {similarity between names with "stemming and edit-distance" + synonym detecting
Edge case: recursive relations in class diagrams were not accounted for at first.

Interesting observation: humans are influenced by the shape of the diagram!
Synonym identification rules, but requires extra verification (it related "FacilityType" to "Card" and "Staff")},
  doi        = {.org/10.21954/ou.ro.00016046},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/pete2006_ApproachToGradingImpreciseDiagrams.pdf:PDF},
  groups     = {incredible!},
  priority   = {prio1},
  readstatus = {read},
  url        = {https://oro.open.ac.uk/90182/1/TR2006-16.pdf},
}

@InProceedings{AlRawashdeh2014,
  author     = {Hazim AlRawashdeh and Sufian Idris and Abdullah Mohd Zin},
  title      = {Using Model Checking Approach for Grading the Semantics of UML Models},
  year       = {2014},
  abstract   = {IEEE Transactions on Magnetics},
  comment    = {is very into using LTL-like formulas and quoting (Nu)SMV, Promela, UPPAAL, ...

produces a hodge podge solution that combines everything, but no quantitative metrics mentioned. Weird paper.},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/Rawashdeh2014_ModelCheckingApproach_GradingSemanticsOfUMLModels.pdf:PDF},
  groups     = {Slightly relevant, Algorithmic},
  keywords   = {UML semantics, Formal representation, Model cannot be put to good use. On the other hand, the symbolic Mapping, Spin, Hugo/RT model checker SMV, is used for verifying the properties, as it},
  readstatus = {read},
  url        = {https://iieng.org/images/proceedings_pdf/8684E0114567.pdf},
}

@InProceedings{Striewe2011,
  author     = {Striewe, Michael and Goedicke, Michael},
  booktitle  = {ITiCSE’11},
  title      = {Automated Checks on UML Diagrams},
  year       = {2011},
  month      = jun,
  pages      = {38--42},
  publisher  = {ACM},
  series     = {ITiCSE ’11},
  abstract   = {exist some systems concerned with this task since the 2000s},
  collection = {ITiCSE ’11},
  comment    = {based on "graph queries which promises to be more flexible".

Has some pretty interesting syntax w.r.t. checking class names, checking syntax. Looks a bit complex though, learning curve could be pretty high for some teachers, and it does not address alternative namings ("Covering different spellings of the same term currently needs tedious lists of alternative strings").},
  doi        = {10.1145/1999747.1999761},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/striewe2011_AutomatedChecksOnUMLDiagrams.pdf:PDF},
  groups     = {Semi-automatic},
  keywords   = {diagrams is not just a matter of right and wrong, but also a matter of quality of the solution. In pure comparisons with},
  priority   = {prio3},
  readstatus = {read},
}

@InProceedings{thomas2004,
  author     = {Pete Thomas},
  title      = {Grading Diagrams Automatically},
  year       = {2004-01-30},
  comment    = {construct a very reduced Box+Arrow graphical language (tiny subset of UML).
The marking is very accurate though, but likely because the question is very simple (though it is not mentioned).

Says nothing about anything, really.},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/thomas2004_GradingDiagramsAutomatically.pdf:PDF},
  groups     = {Bad-ish},
  priority   = {prio3},
  readstatus = {read},
  url        = {https://oro.open.ac.uk/90155/1/2004_01.pdf},
}

@InProceedings{thomas2009,
  author     = {Thomas, Pete and Smith, Neil and Waugh, Kevin},
  booktitle  = {Proceedings of the IADIS International Conference on e-Learning},
  title      = {Automatically Assessing Diagrams},
  year       = {2009},
  comment    = {update to 2008, small improvements: "since been updated to  deal with more situtations and shows a small improvement" (p.268).

One discrepancy (outlier) was about  duplicate names ("two entities with the same label" (p.268)). GOOD POINT FOR GRADING!

"If a ‘new’ solution comes to light, it is straightforward to remark all diagrams, something that is rarely done with human marking because of the cost" (p.269) <-- QUOTE

Kind of summary of previous work of Thomas.},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/thomas2009_AutomaticallyAssessingDiagrams.pdf:PDF},
  groups     = {Algorithmic},
  priority   = {prio1},
  readstatus = {read},
  url        = {https://www.researchgate.net/profile/Pete-Thomas/publication/42799920_Automatically_assessing_diagrams/links/0fcfd5060076dd8ba2000000/Automatically-assessing-diagrams.pdf},
}

@Article{thomas2008,
  author     = {Thomas, Pete and Smith, Neil and Waugh, Kevin},
  journal    = {Learning, Media and Technology},
  title      = {Automatically assessing graph‐based diagrams},
  year       = {2008},
  issn       = {1743-9892},
  number     = {3},
  pages      = {249--267},
  volume     = {33},
  comment    = {details about synonym matching, aggregation ("For example, a many-to-many relationship may be replaced by an equivalent pair of one-to-many relationships with a new entity type in common")

GOOD: very nice figure 4: The 5-stage framework for automatic imprecise diagram interpretation.

"known variability in human marking" (p.10)

Very nice theory for dealing with inheritance checking, many-to-many checking, .... Incredible work!

"The interpretation phase has been applied to the marking of ERDs and compared with human markers has been shown to be effective Figure 11. The marker’s redrawing assistant. Learning, across a large number of ERDs." (p.265)},
  doi        = {10.1080/17439880802324251},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/thomas2008_AutomaticallyAssessingGraphBasedDiagrams.pdf:PDF},
  groups     = {incredible!},
  priority   = {prio1},
  publisher  = {Taylor & Francis},
  readstatus = {read},
  url        = {https://www.tandfonline.com/doi/pdf/10.1080/17439880802324251},
}

@InProceedings{Vachharajani2014,
  author     = {Vinay Vachharajani and Jyoti Pareek},
  booktitle  = {International Journal of Computer Applications (0975 – 8887)},
  title      = {A Proposed Architecture for Automated Assessment of Use Case Diagrams},
  year       = {2014},
  number     = {4},
  volume     = {108},
  abstract   = {The assessment part could be a deterrent as far as willingness},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/Vachharajani2014_ProposedArchitectureAssessmentUseCaseDiagrams.pdf:PDF},
  groups     = {Advice - not implementation},
  keywords   = {In the remainder of this paper, section 2 discusses the work},
  readstatus = {read},
  url        = {https://www.academia.edu/download/67672696/pxc3900193.pdf},
}

@InProceedings{Williams2025,
  author     = {Williams, Eldon and Xing, Guangming},
  booktitle  = {Proceedings of the 56th ACM Technical Symposium on Computer Science Education V. 2},
  title      = {GRADE: Grading and Assessment of Database ER Diagrams},
  year       = {2025},
  month      = feb,
  number     = {Technical},
  pages      = {1659--1660},
  publisher  = {ACM},
  series     = {SIGCSE TS 2025},
  abstract   = {approach. GRADE is designed to offer a Chen-style [1] ER model-
Entity-relationship (ER) modeling is a crucial component of data- ing interface, providing students with an intuitive environment to
base education, allowing students to conceptualize real-world prob- construct ER diagrams.
lems and translate them into database schemas. Existing tools, such
as AutoER, provide automated grading for UML-style diagrams but 2 Tool Overview
often lack flexibility in representing more complex relationships. GRADE offers an intuitive, web-based interface where students can
In this poster, we introduce GRADE(Grading and Assessment of construct Chen-style ER diagrams. The key design goal of GRADE
Database ER Diagrams), a tool designed to support the creation and is to balance flexibility in diagram construction with consistency
automatic grading of ER diagrams. Unlike UML-based alternatives, in the automated grading process. Professors have full control over
GRADE focuses on providing a flexible environment for students the creation of custom ER diagram questions, enabling them to
to construct ER diagrams, improving their understanding of core define specific entities, relationships, and attributes. The figure
database concepts. GRADE also offers real-time feedback and scal- below illustrates the basic usage of the GRADE tool. The GRADE
ability in assessments. This poster outlines the features of GRADE
and discusses its potential impact on database education.},
  collection = {SIGCSE TS 2025},
  doi        = {10.1145/3641555.3705191},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/williams2025_GRADE_Database_ER_diagrams.pdf:PDF},
  readstatus = {skimmed},
}

@InProceedings{Schoettle2015,
  author     = {Matthias Schöttle and Nishanth Thimmegowda and Omar Alam and Jörg Kienzle and Gunter Mussbacher},
  booktitle  = {Companion Proceedings of the 14th International Conference on Modularity},
  title      = {Feature Modelling and Traceability for Concern-Driven Software Development with TouchCORE},
  year       = {2015},
  month      = mar,
  pages      = {11--14},
  publisher  = {ACM},
  series     = {Modularity ’15},
  abstract   = {port for concern-driven software development based on a common
This demonstration paper presents TouchCORE, a multi-touch en- metamodel [5].
abled software design modelling tool aimed at developing scal- As of early 2015, TouchCORE now provides full support for
able and reusable software design models following the concern- concern-driven software development as outlined in this paper. Sec-
driven software development paradigm. After a quick review of tion 2 briefly recalls the main concepts of concern-driven software
concern-orientation, this paper primarily focusses on the new fea- development. Section 3 describes TouchCORE’s support for fea-
tures that were added to TouchCORE since the last demonstration ture model and impact model visualization and assessment tar-
at Modularity 2014 (were the tool was still called TouchRAM). geted at concern designers as well as concern users. Section 4 ex-
TouchCORE now provides full support for concern-orientation. plains how traceability support was added to the CORE (Concern-
This includes support for feature model editing and different modes Oriented REuse) metamodel, which defines the key concepts of
for feature model and impact model visualization and assessment concern-driven software development, to allow visualization of the
to best assist the concern designers as well as the concern users. To crosscutting nature of concerns in woven models. The last section
help the modeller understand the interactions between concerns, draws our conclusions.
TouchCORE now also collects tracing information when concerns
are reused and stores that information with the woven models. This
makes it possible to visualize from which concern(s) a model ele- 2. Concern-Driven Software Development
ment in the woven model has originated.},
  collection = {Modularity ’15},
  doi        = {10.1145/2735386.2735922},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/schottle2015_TouchCORE.pdf:PDF},
  keywords   = {concern-driven software development, reuse, feature models, impact models, traceability. groups together software artifacts (models and code, henceforth},
  readstatus = {skimmed},
}

@InProceedings{Foss2022a,
  author     = {Sarah Foss and Tatiana Urazova and Ramon Lawrence},
  booktitle  = {Proceedings of the 53rd ACM Technical Symposium on Computer Science Education},
  title      = {Automatic Generation and Marking of UML Database Design Diagrams},
  year       = {2022},
  month      = feb,
  number     = {solutions.},
  pages      = {626--632},
  publisher  = {ACM},
  series     = {SIGCSE 2022},
  abstract   = {systems support the generation and auto-marking of questions
Interactive question systems improve student engagement and pro- [11, 15, 34] for practice and improving learning outcomes.
vide opportunities for increased practice and skill mastery. Develop- Database design and modeling are key concepts taught in data-
ing database design diagrams is a key skill for database courses, but base and software engineering courses. Design exercises are often
providing evaluation feedback is time-consuming for instructors challenging for students and additional practice helps learning. This
and accurate auto-grading is challenging due to the variability of work focuses on questions where the goal is to produce a design
student answers especially when labeling diagram components. diagram for a database that captures the requirements discussed in
This work presents a system for the automatic creation and real- a written text. Evaluating design diagrams is complex due to the
time evaluation of database design questions using UML diagrams. lack of testing frameworks, diversity of correct answers, and inter-
Students directly interact with the question text, and the system preting the semantics of the diagram and its concepts. Prior work
continuously generates a visual representation of their answer as examined automatic evaluation of design diagrams using image
well as provides immediate feedback at any time. By utilizing a processing [32], syntactic, structural, and semantic analysis [7], and
web-based, customizable user interface, the system supports precise machine learning [9]. These systems provide evaluations similar to
marking and the ability to practice variants of design questions to human markers for instructor developed questions.
mastery. A classroom evaluation demonstrates high student satisfac- The contribution of this work is a question generation and evalu-
tion compared to traditional UML design questions and preference ation system for UML database design diagrams and its evaluation
for using the software to improve their learning outcomes. in a database course. The system, called AutoER, supports both},
  collection = {SIGCSE 2022},
  doi        = {10.1145/3478431.3499376},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/foss2022_AutomaticGenerationMarkingUMLDiagrams.pdf:PDF},
  keywords   = {operation as a stand-alone system or integrated with a learning management system. A unique aspect of the user interface is that},
  readstatus = {skimmed},
}

@Article{he2025,
  author     = {Horace He and Thinking Machines},
  title      = {Defeating Nondeterminism in LLM Inference},
  year       = {2025-09-10},
  comment    = {LLMs with temperature = 0 can still be nondeterminstic ("run-to-run determinism") because "kernels add numbers in different orders" (concurrency + floating point)},
  groups     = {LLM downsides},
  readstatus = {read},
  url        = {https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/},
}

@Article{brenndoerfer2025,
  author     = {Michael Brenndoerfer},
  title      = {Why Temperature=0 Doesn't Guarantee Determinism in LLMs},
  year       = {2025},
  comment    = {Temperature
= 0 in language models aims to eliminate randomness by always selecting
the most probable next token (i.e., greedy decoding).
Despite this, real-world LLMs still produce varying outputs due to
factors like:

* Floating-point precision limitations
* Hardware and parallel processing variability
* Decoding tie-breakers
* Other sampling parameters like top-k or top-p that may still introduce variation

Additional nondeterminism comes from model and infrastructure complexity, including:

* Mixture-of-Experts ([MoE](https://mbrenndoerfer.com/writing/mixture-of-experts-sparse-activation)) architectures
* Nondeterministic operations in frameworks like PyTorch or TensorFlow
* Multi-GPU sharding
* Batching, [load balancing](https://mbrenndoerfer.com/writing/scaling-ai-agents-performance-cost-optimization), and cross-server variability

While
temperature=0 increases consistency, true repeatability requires strict
control over hardware, software, and decoding settings.
Ultimately, these nuances explain why outputs can still differ, even
when no randomness is explicitly introduced.},
  groups     = {LLM downsides},
  readstatus = {read},
  url        = {https://mbrenndoerfer.com/writing/why-llms-are-not-deterministic},
}

@InProceedings{atil2025,
  author     = {Berk Atil and Sarp Aykent and Alexa Chittams and Lisheng Fu and Rebecca J. Passonneau and Evan Radcliffe and Guru Rajan Rajagopal and Adam Sloan and Tomasz Tudrej and Ferhan Ture and Zhe Wu and Lixinyu Xu and Breck Baldwin},
  title      = {Non-Determinism of “Deterministic” LLM Settings},
  year       = {2025-04-02},
  comment    = {"Although the use of multiple GPUs introduces some randomness (...), it can be eliminated by setting random seeds (...). However, engineering optimizations to run LLMs faster (...) might lead to nondeterministic behavior."},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/Atil2025_NonDeterminismOfDeterministicLLMSettings.pdf:PDF},
  groups     = {LLM downsides},
  readstatus = {skimmed},
  url        = {https://arxiv.org/pdf/2408.04667},
}

@InProceedings{Hasker2011,
  author     = {Robert W. Hasker},
  title      = {UMLGrader: An automated class diagram grader},
  year       = {2011},
  abstract   = {J. Comput. Sci. Coll. 2011.27:47-54},
  comment    = {system designed to provide automated feedback to students on UML class diagrams - FOR FEEDBACK (is more strict than exam grader would be) 

- Elements are strictly matched by set of possible names (interesting, but not very scalable - see other word matching algorithms)
- Attributes match by substring (split up into words/parts of words and then match)
- Classes must match by whole name
- Association names (when present) also matched on whole name
- Associations/multiplicities are IGNORED when not present in sample solution.
(p.50)

Extra niceities for feedback (reducing points for nondescript names, misspelled words, ...)

Future work: multiple target/sample solutions, checking named associations, navigability, containment, types, evaluating more types of diagrams.

Interesting alternative setup to Thomas (2005-2009) papers and Anas (2021) graph-related grading.},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/Hasker2011_UMLGrader.pdf:PDF},
  priority   = {prio2},
  readstatus = {read},
  url        = {https://dl.acm.org/doi/abs/10.5555/2037151.2037163},
}

@InProceedings{smith2013,
  author     = {Neil Smith and Pete Thomas and Kevin Waugh},
  booktitle  = {2013 Learning and Teaching in Computing and Engineering},
  title      = {Automatic Grading of Free-Form Diagrams with Label Hypernymy},
  year       = {2013},
  month      = mar,
  pages      = {136--142},
  publisher  = {IEEE},
  abstract   = {2013 Learning and Teaching in Computing and Engineering;2013; ; ;10.1109/LaTiCE.2013.33},
  comment    = {"This paper presents the only system we know of capable of automatically grading free-form diagrammatic answers"

Grading ERD, UML, Bio diagrams, very interesting!
Also contains good definitions.

Grade by comparing Minimal Meaningful Units (MMUs, combinations of elements and connections) with a similarity measure in [0..1].
Like other papers, hint to difficulty of comparing human text (labels).

600 (!) diagrams tested.},
  doi        = {10.1109/latice.2013.33},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/SmithThomas2013_AutomaticGradingFree-FormDiagrams_LabelHypernymy.pdf:PDF},
  groups     = {incredible!},
  keywords   = {automatic grading; diagrams; e-assessment that do not match the expected diagram either because they do},
  priority   = {prio1},
  readstatus = {read},
}

@Misc{Meadows2005,
  author     = {Michelle Meadows and Lucy Billington},
  title      = {A Review Of THe Literature On Marking Reliability},
  year       = {2005-03},
  comment    = {"The literature reviewed has made clear the inherent unreliability associated with assessment in general, and associated with marking in particular. The extent of this unreliability may vary across subjects and assessment formats, and may be improved through marker training, attention to marking schemes and so on. Nonetheless while particular assessment formats, for example essays, are valued by those involved in education there has to be an acceptance that the marks or grades that candidates receive will not be perfectly reliable." (p.68)},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/Meadows2005_ReviewLiteratureMarkingReliability.pdf:PDF},
  groups     = {Man.Grad.Review},
  readstatus = {skimmed},
  url        = {https://assets.publishing.service.gov.uk/media/5a820a57e5274a2e87dc0d5a/0505_Meadows_and_Billington_CERP_RP.pdf},
}

@Misc{utml,
  title = {UTML - old student repository},
  url   = {https://github.com/andrewjh9/UTML},
}

@TechReport{Wang2025,
  author     = {Chong Wang and Beian Wang and Peng Liang and Jie Liang;},
  title      = {Assessing UML Diagrams by GPT: Implications for Education},
  year       = {2025},
  comment    = {- evaluation criteria to quantitatively assess UML Use Case, Class, Sequence Diagrams
- 40-participant study - 5 types of evaluation discrepancy, analyse competency of GPT-grading.

Criteria-based, follow role-play prompting, triggering Chain-of-Thought processes.

Temperature is not at 0! No deterministic grading. One run, no seed.

"GPT is effective (...) However, GPT usually generates lower scores than human experts (...). In addition, the performance of GPT evaluating different types of UML diagrams"

Example hallucination: "In the evaluation based on UC4, GPT deducts points for missing relationships between specified actors and use cases, but theses relationships existed in the UML use case" (p.13)
diagram"},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/wang2025AssessingUMLDiagramsGPT.pdf:PDF},
  groups     = {LLM},
  keywords   = {UML Diagram, GPT, Model Assessment, Software Modeling Education},
  readstatus = {skimmed},
  url        = {https://www.researchgate.net/publication/397720325_Assessing_UML_Diagrams_by_GPT_Implications_for_Education},
}

@Article{Ferraris2025,
  author      = {Andrea Filippo Ferraris and Davide Audrito and Luigi Di Caro and Cristina Poncibò},
  journal     = {Cambridge Forum on AI: Law and Governance},
  title       = {{T}he architecture of language: {U}nderstanding the mechanics behind {L}{L}{M}s},
  year        = {2025},
  pages       = {1–19},
  volume      = {1},
  abstract    = {Large language models (LLMs) have significantly advanced artificial intelligence (AI) and natural language
processing (NLP) by excelling in tasks like text generation, machine translation, question answering and
sentiment analysis, often rivaling human performance. This paper reviews LLMs’ foundations, advance-
ments and applications, beginning with the transformative transformer architecture, which improved on
earlier models like recurrent neural networks and convolutional neural networks through self-attention
mechanisms that capture long-range dependencies and contextual relationships. Key innovations such
as masked language modeling and causal language modeling underpin leading models like Bidirectional
encoder representations from transformers (BERT) and the Generative Pre-trained Transformer (GPT)
series. The paper highlights scaling laws, model size increases and advanced training techniques that have
driven LLMs’ growth. It also explores methodologies to enhance their precision and adaptability, includ-
ing parameter-efficient fine-tuning and prompt engineering. Challenges like high computational demands,
biases and hallucinations are addressed, with solutions such as retrieval-augmented generation to improve
factual accuracy. By discussing LLMs’ strengths, limitations and transformative potential, this paper pro-
vides researchers, practitioners and students with a comprehensive understanding. It underscores the
importance of ongoing research to improve efficiency, manage ethical concerns and shape the future of
AI and language technologies.},
  booktitle   = {Cambridge Forum on AI: Law and Governance},
  doi         = {10.1017/cfl.2024.16},
  file        = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/cambridgeArchitectureOfLLMs.pdf:PDF},
  groups      = {LLM},
  institution = {Cambridge Forum on AI: Law and Governance},
  keywords    = {large language models (LLMs); artificial intelligence (AI); natural language processing (NLP)},
  publisher   = {Cambridge Univesrity Press},
  readstatus  = {skimmed},
  series      = {11},
  url         = {https://www.cambridge.org/core/journals/cambridge-forum-on-ai-law-and-governance/article/architecture-of-language-understanding-the-mechanics-behind-llms/E3DDEFB9C04883733380E04331D6F782},
}

@Misc{utml-internal,
  author = {David Huistra},
  title  = {UTML - internal GitLab repository},
  url    = {https://gitlab.utwente.nl/ewi/eduapps/UTML/},
}

@Misc{xmi,
  author     = {OMG},
  title      = {XMI specification},
  groups     = {Other},
  readstatus = {read},
  url        = {https://www.omg.org/spec/XMI},
}

@Misc{ibm-rational-rose,
  author     = {IBM},
  title      = {Rational Rose},
  groups     = {Other},
  readstatus = {read},
  url        = {https://www.ibm.com/support/pages/ibm-rational-rose-enterprise-7004-fix-pack-4-7000},
}

@InProceedings{smith2004,
  author     = {Neil Smith and Pete Thomas and Kevin Waugh},
  booktitle  = {Diagrammatic Representation and Inference},
  title      = {Interpreting imprecise diagrams},
  year       = {2004},
  month      = mar,
  pages      = {239–241},
  series     = {International Conference on Theory and Application of Diagrams},
  doi        = {10.1007/978-3-540-25931-2_24},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/smith2004_InterpretingImpreciseDiagrams.pdf:PDF},
  groups     = {Algorithmic, Advice - not implementation},
  priority   = {prio1},
  readstatus = {read},
  url        = {https://link.springer.com/chapter/10.1007/978-3-540-25931-2_24},
}

@InProceedings{thomas2011,
  author     = {Pete Thomas and Kevin Waugh and Neil Smith},
  booktitle  = {Innovation in Teaching and Learning in Information and Computer Sciences},
  title      = {Generalised Diagramming Tools with Automatic Marking},
  year       = {2011},
  doi        = {10.11120/ital.2011.10010022},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/thomas2011_GeneralisedMarkingTools.pdf:PDF},
  groups     = {incredible!},
  priority   = {prio2},
  readstatus = {read},
  url        = {https://www.tandfonline.com/doi/full/10.11120/ital.2011.10010022},
}

@InProceedings{ranjan2024,
  author     = {Rajesh Ranjan and Shailja Gupta and Surya Narayan Singh},
  title      = {A Comprehensive Survey of Bias in LLMs: Current Landscape and Future Directions},
  year       = {2024},
  doi        = {10.48550/arXiv.2409.16430},
  file       = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/ranjan2024_SurveyOfBiasIinLLMs.pdf:PDF},
  groups     = {LLM downsides},
  keywords   = {Large Language Model (LLM), Biases, Natural Language Processing (NLP), Bias Mitigation, Artificial Intelligence and Fairness in AI},
  readstatus = {skimmed},
  url        = {https://arxiv.org/abs/2409.16430},
}

@MastersThesis{osinga2024,
  author     = {Osinga, Douwe},
  school     = {University of Twente},
  title      = {Combining Dynamic and Static Analysis for the Automation of Grading Programming Exams},
  year       = {2024},
  type       = {{B}achelor thesis},
  groups     = {Algorithmic},
  readstatus = {read},
  url        = {https://github.com/osingaatje/ut-bachelor-thesis},
}

@InProceedings{wei2023,
  author = {Jason Wei and Xuezhi Wang and Dale Schuurmans and Maarten Bosma and Brian Ichter and Fei Xia and Ed H. Chi and Quoc V. Le and Denny Zhou},
  title  = {Chain-of-Thought Prompting Elicits Reasoning in Large Language Models},
  year   = {2023},
  file   = {:/home/douwe/Documents/UNIVERSITEIT_TWENTE/MASTERS/Y2GRAD/SOURCES/wei2023_chainofthought.pdf:PDF},
  groups = {LLM},
  url    = {https://arxiv.org/abs/2201.11903},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:Autograding\;2\;1\;0x008000ff\;COMPUTER\;\;;
2 StaticGroup:LLM\;2\;1\;0xf2f2f2ff\;SLOT_MACHINE_OUTLINE\;\;;
3 StaticGroup:bad/not worth reading\;2\;1\;0x800000ff\;SLOT_MACHINE_OUTLINE\;\;;
3 StaticGroup:LLM downsides\;2\;1\;0x666600ff\;SLOT_MACHINE\;\;;
2 StaticGroup:Review\;2\;1\;0x804d80ff\;FOUNTAIN_PEN_TIP\;\;;
2 StaticGroup:Advice - not implementation\;2\;0\;0x808080ff\;ANNOUNCEMENT\;\;;
2 StaticGroup:Algorithmic\;2\;1\;0x800080ff\;GMI_360\;\;;
3 StaticGroup:Semi-automatic\;2\;0\;0xb34d1aff\;STATE_MACHINE\;\;;
3 StaticGroup:incredible!\;2\;0\;0x00ff00ff\;GMI_360\;\;;
3 StaticGroup:Bad-ish\;2\;1\;0x800000ff\;GMI_360\;\;;
1 StaticGroup:NOT autograding\;2\;0\;\;MDI_HUMAN\;\;;
1 StaticGroup:Slightly relevant\;2\;1\;\;AIRLINE_SEAT_INDIVIDUAL_SUITE\;\;;
1 StaticGroup:ManualGradingReview\;2\;1\;\;PERSON_OUTLINE\;\;;
2 StaticGroup:Man.Grad.Review\;2\;0\;0x000080ff\;BOOK_EDIT_OUTLINE\;\;;
1 StaticGroup:Other\;2\;1\;0x808000ff\;ARROW_DROP_DOWN\;\;;
}
